{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WpgPXIBt87lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOUOQOrFs3zn"
      },
      "source": [
        "## RL with Soft Actor Critic (SAC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-22T12:28:24.535576Z",
          "iopub.status.busy": "2023-12-22T12:28:24.535015Z",
          "iopub.status.idle": "2023-12-22T12:28:37.687013Z",
          "shell.execute_reply": "2023-12-22T12:28:37.686082Z"
        },
        "id": "fskoLlB-AZ9j"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install pyvirtualdisplay\n",
        "\n",
        "!pip install matplotlib\n",
        "!pip install tf-agents[reverb]\n",
        "!pip install pyglet xvfbwrapper\n",
        "!pip install pybullet\n",
        "!pip install tf-keras\n",
        "\n",
        "\n",
        "!pip install dm-reverb[tensorflow]\n",
        "!sudo apt-get install swig\n",
        "!pip install gym[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-22T12:28:37.691361Z",
          "iopub.status.busy": "2023-12-22T12:28:37.691063Z",
          "iopub.status.idle": "2023-12-22T12:28:37.695220Z",
          "shell.execute_reply": "2023-12-22T12:28:37.694410Z"
        },
        "id": "WPuD0bMEY9Iz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Keep using keras-2 (tf-keras) rather than keras-3 (keras).\n",
        "os.environ['TF_USE_LEGACY_KERAS'] = '1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u9QVVsShC9X"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-22T12:28:37.698536Z",
          "iopub.status.busy": "2023-12-22T12:28:37.698284Z",
          "iopub.status.idle": "2023-12-22T12:28:40.957491Z",
          "shell.execute_reply": "2023-12-22T12:28:40.956807Z"
        },
        "id": "sMitx5qSgJk1"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import reverb\n",
        "import tempfile\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.ddpg import critic_network\n",
        "from tf_agents.agents.sac import sac_agent\n",
        "from tf_agents.agents.sac import tanh_normal_projection_network\n",
        "from tf_agents.environments import suite_pybullet, suite_gym\n",
        "from tf_agents.metrics import py_metrics\n",
        "from tf_agents.networks import actor_distribution_network\n",
        "from tf_agents.policies import greedy_policy\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_py_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.train import actor\n",
        "from tf_agents.train import learner\n",
        "from tf_agents.train import triggers\n",
        "from tf_agents.train.utils import spec_utils\n",
        "from tf_agents.train.utils import strategy_utils\n",
        "from tf_agents.train.utils import train_utils\n",
        "\n",
        "tempdir = tempfile.gettempdir()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Set up a virtual display for rendering OpenAI gym environments.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmC0NDhdLIKY"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#env names\n",
        "# should be a env with continuous action space for SAC\n",
        "\n",
        "PY_BULLET_ENVS = {\n",
        "    \"MinitaurBulletEnv-v0\" : {\n",
        "        \"env_kwargs\": None,\n",
        "         \"description\": f\"\"\"This environment is a empty land with a robot, the goal is to control the Minitaur robot and have it move forward as fast as possible.\"\"\"\n",
        "    },\n",
        "}\n",
        "\n",
        "GYM_ENVS = {\n",
        "    \"LunarLander-v2\" : {\n",
        "         \"env_kwarg\": { \"continuous\": True, }, #\"gravity\": -10.0, \"enable_wind\": False, \"wind_power\": 15.0, \"turbulence_power\": 1.5,\n",
        "         \"description\": f\"\"\"This environment is a classic rocket trajectory optimization problem, the goal is to help the pad to land safely. [full](https://www.gymlibrary.dev/environments/box2d/lunar_lander/)\"\"\"\n",
        "    },\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "5hZfoRHLD4A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "env_name = \"LunarLander-v2\" # @param [\"LunarLander-v2\",  \"MinitaurBulletEnv-v0\"]\n",
        "\n",
        "\n",
        "# Use \"num_iterations = 1e6\" for better results (2 hrs)\n",
        "# 1e5 is just so this doesn't take too long (1 hr)\n",
        "num_iterations = 100_000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 10_000 # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration = 1 # @param {type:\"integer\"}\n",
        "replay_buffer_capacity = 10000 # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 256 # @param {type:\"integer\"}\n",
        "\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### Agent hyperparams:\n",
        "# Agent hyperparams\n",
        "critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "alpha_learning_rate = 3e-4 # @param {type:\"number\"}\n",
        "target_update_tau = 0.005 # @param {type:\"number\"}\n",
        "target_update_period = 1 # @param {type:\"number\"}\n",
        "gamma = 0.99 # @param {type:\"number\"}\n",
        "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
        "\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### Actor NN hyperparams:\n",
        "# actor NN hyperparams\n",
        "actor_fc_layer_params = (256,  256) # @param\n",
        "actor_dropout_layer_params = 0.15     # @param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "\n",
        "# actor_dropout_layer_params = [actor_dropout_layer_params] * len(actor_fc_layer_params) if (not actor_fc_layer_params == None) else None\n",
        "actor_dropout_layer_params = None\n",
        "\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### Critic NN hyperparams:\n",
        "# critic NN hyperparams\n",
        "critic_action_fc_layer_params = (64, ) # @param\n",
        "critic_observation_fc_layer_params = (64, ) # @param\n",
        "critic_joint_fc_layer_params = (128, 256)   # @param\n",
        "critic_observation_dropout_layer_params = 0.15     # @param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "critic_action_dropout_layer_params = 0.15          # @param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "\n",
        "# critic_observation_dropout_layer_params = [critic_observation_dropout_layer_params] * len(critic_observation_fc_layer_params) if (not critic_observation_fc_layer_params == None) else None\n",
        "# critic_action_dropout_layer_params      = [critic_action_dropout_layer_params] * len(critic_action_fc_layer_params) if (not critic_action_fc_layer_params == None) else None\n",
        "\n",
        "critic_observation_dropout_layer_params = None\n",
        "critic_action_dropout_layer_params = None\n",
        "\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### training eval and log params:\n",
        "#training eval and log params\n",
        "log_interval = 5000 # @param {type:\"integer\"}\n",
        "num_eval_episodes = 20 # @param {type:\"integer\"}\n",
        "eval_interval = 10000 # @param {type:\"integer\"}\n",
        "policy_save_interval = 5000 # @param {type:\"integer\"}\n",
        "\n",
        "\n",
        "#GPU support\n",
        "use_gpu  = True #@param {type:\"boolean\"}\n",
        "strategy = strategy_utils.get_strategy(tpu=False, use_gpu=use_gpu)"
      ],
      "metadata": {
        "id": "Q7REo4h91BW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMsJC3DEgI0x"
      },
      "source": [
        "## Environment\n",
        "\n",
        "load and examine the environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NykZcvR-jV2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-22T12:28:40.969638Z",
          "iopub.status.busy": "2023-12-22T12:28:40.969069Z",
          "iopub.status.idle": "2023-12-22T12:28:41.558351Z",
          "shell.execute_reply": "2023-12-22T12:28:41.557717Z"
        },
        "id": "RlO7WIQHu_7D"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "#load env\n",
        "current_suite = None\n",
        "ENV_CONFIG = None\n",
        "try:\n",
        "  if env_name in PY_BULLET_ENVS:\n",
        "    env = suite_pybullet.load(env_name)\n",
        "    current_suite = suite_pybullet\n",
        "    ENV_CONFIG = PY_BULLET_ENVS[env_name]\n",
        "    print(f\"\"\" {env_name} loaded from suite_pybullet \"\"\")\n",
        "  elif env_name in GYM_ENVS:\n",
        "    env = suite_gym.load(env_name, gym_kwargs = GYM_ENVS[env_name][\"env_kwarg\"] )\n",
        "    current_suite = suite_gym\n",
        "    ENV_CONFIG = GYM_ENVS[env_name]\n",
        "    print(f\"\"\" {env_name} loaded from suite_gym \"\"\")\n",
        "  else:\n",
        "    raise Exception\n",
        "\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  print( f\"\"\"Something went wrong:: {env_name} is not present in either Pybullet or Gym \"\"\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print( f\"\"\" ** Description ** :: { ENV_CONFIG.get(\"description\") } \\n\\n\"\"\" )\n",
        "\n",
        "#examine spec\n",
        "env.reset()\n",
        "time_step_spec = env.time_step_spec().observation\n",
        "action_spec = env.action_spec()\n",
        "\n",
        "print('Observation Spec:')\n",
        "print(time_step_spec)\n",
        "print('\\nAction Spec:')\n",
        "print(action_spec)\n",
        "\n",
        "\n",
        "#render env\n",
        "print('\\n', '--------------------'*5, 'render environment')\n",
        "PIL.Image.fromarray(env.render())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_random_action(seed = None): return tf.random.uniform(shape=action_spec.shape, minval=action_spec.minimum, maxval=action_spec.maximum, dtype=action_spec.dtype, seed=seed, name=None)\n",
        "\n",
        "time_step = env.reset()\n",
        "print('Time step:')\n",
        "print(time_step)\n",
        "\n",
        "action = get_random_action()\n",
        "\n",
        "next_time_step = env.step(action.numpy())\n",
        "print('Next time step:')\n",
        "print(next_time_step)\n",
        "\n",
        "\n",
        "#define training and evel env\n",
        "collect_env = current_suite.load(env_name, gym_kwargs = ENV_CONFIG[\"env_kwarg\"] )\n",
        "eval_env    = current_suite.load(env_name, gym_kwargs = ENV_CONFIG[\"env_kwarg\"] )"
      ],
      "metadata": {
        "id": "tIBZYXGXFc4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-22T12:28:41.572146Z",
          "iopub.status.busy": "2023-12-22T12:28:41.571916Z",
          "iopub.status.idle": "2023-12-22T12:28:41.850893Z",
          "shell.execute_reply": "2023-12-22T12:28:41.850197Z"
        },
        "id": "Xp-Y4mD6eDhF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9lW_OZYFR8A"
      },
      "source": [
        "## Agent [Critic + Actor]\n",
        "\n",
        "The `CriticNetwork` will give an estimation of Q(s,a)\n",
        "\n",
        "\n",
        "The `ActorNetwork` will predict parameters for a tanh-squashed [MultivariateNormalDiag](https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalDiag) distribution. This distribution will then be sampled, conditioned on the current observation, whenever we need to generate actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-22T12:28:45.071652Z",
          "iopub.status.busy": "2023-12-22T12:28:45.070977Z",
          "iopub.status.idle": "2023-12-22T12:28:45.092816Z",
          "shell.execute_reply": "2023-12-22T12:28:45.092180Z"
        },
        "id": "TgkdEPg_muzV"
      },
      "outputs": [],
      "source": [
        "observation_spec, action_spec, time_step_spec = ( spec_utils.get_tensor_specs(collect_env) )\n",
        "\n",
        "\n",
        "#critic NN gives an estimation of Q(s,a), for a couple {state, action}, outputs how good the action is\n",
        "with strategy.scope():\n",
        "  critic_net = critic_network.CriticNetwork(\n",
        "        input_tensor_spec=(observation_spec, action_spec),\n",
        "\n",
        "        observation_fc_layer_params=critic_observation_fc_layer_params, #None\n",
        "        observation_dropout_layer_params=critic_observation_dropout_layer_params, #None\n",
        "\n",
        "        action_fc_layer_params=critic_action_fc_layer_params, #None\n",
        "        action_dropout_layer_params = critic_action_dropout_layer_params, #None\n",
        "\n",
        "        joint_fc_layer_params=critic_joint_fc_layer_params,\n",
        "        kernel_initializer='glorot_uniform',\n",
        "        last_kernel_initializer='glorot_uniform',\n",
        "\n",
        "        output_activation_fn =  None, #use tf.keras.activations.tanh so the Q(s, a) is bounded between [-1, +1]\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with strategy.scope():\n",
        "  actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
        "      input_tensor_spec  = observation_spec,\n",
        "      output_tensor_spec = action_spec,\n",
        "      fc_layer_params=actor_fc_layer_params,\n",
        "      continuous_projection_net=(tanh_normal_projection_network.TanhNormalProjectionNetwork)\n",
        "      )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#create an agent from [critic + actor]\n",
        "with strategy.scope():\n",
        "  train_step = train_utils.create_train_step()\n",
        "\n",
        "  tf_agent = sac_agent.SacAgent(\n",
        "        time_step_spec,\n",
        "        action_spec,\n",
        "        actor_network=actor_net,\n",
        "        critic_network=critic_net,\n",
        "        actor_optimizer =tf.keras.optimizers.Adam(learning_rate=actor_learning_rate),\n",
        "        critic_optimizer=tf.keras.optimizers.Adam(learning_rate=critic_learning_rate),\n",
        "        alpha_optimizer= tf.keras.optimizers.Adam(learning_rate=alpha_learning_rate),\n",
        "        target_update_tau=target_update_tau,\n",
        "        target_update_period=target_update_period,\n",
        "        td_errors_loss_fn=tf.math.squared_difference,\n",
        "        gamma=gamma,\n",
        "        reward_scale_factor=reward_scale_factor,\n",
        "        train_step_counter=train_step)\n",
        "\n",
        "  tf_agent.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-22T12:28:45.126696Z",
          "iopub.status.busy": "2023-12-22T12:28:45.126167Z",
          "iopub.status.idle": "2023-12-22T12:28:45.881456Z",
          "shell.execute_reply": "2023-12-22T12:28:45.880707Z"
        },
        "id": "jbY4yrjTEyc9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLva6g2jdWgr"
      },
      "source": [
        "## Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-22T12:28:45.885416Z",
          "iopub.status.busy": "2023-12-22T12:28:45.884996Z",
          "iopub.status.idle": "2023-12-22T12:28:45.894434Z",
          "shell.execute_reply": "2023-12-22T12:28:45.893761Z"
        },
        "id": "vX2zGUWJGWAl"
      },
      "outputs": [],
      "source": [
        "table_name = 'uniform_table'\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_capacity,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1))\n",
        "\n",
        "reverb_server = reverb.Server([table])\n",
        "\n",
        "\n",
        "\n",
        "reverb_replay = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    tf_agent.collect_data_spec,\n",
        "    sequence_length=2, #set to 2 Since the SAC Agent needs both the current and next observation to compute the loss\n",
        "    table_name=table_name,\n",
        "    local_server=reverb_server)\n",
        "\n",
        "\n",
        "# reverb replay buffer to tf dataset: We will pass this to the Learner to sample experiences for training.\n",
        "dataset = reverb_replay.as_dataset( sample_batch_size=batch_size, num_steps=2).prefetch(100)\n",
        "experience_dataset_fn = lambda: dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-22T12:28:45.907387Z",
          "iopub.status.busy": "2023-12-22T12:28:45.906834Z",
          "iopub.status.idle": "2023-12-22T12:28:46.224611Z",
          "shell.execute_reply": "2023-12-22T12:28:46.223903Z"
        },
        "id": "ba7bilizt_qW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0KLrEPwkn5x"
      },
      "source": [
        "## Policies\n",
        "\n",
        "In TF-Agents, policies represent the standard notion of policies in RL: given a `time_step` produce an action or a distribution over actions. The main method is `policy_step = policy.step(time_step)` where `policy_step` is a named tuple `PolicyStep(action, state, info)`.  The `policy_step.action` is the `action` to be applied to the environment, `state` represents the state for stateful (RNN) policies and `info` may contain auxiliary information such as log probabilities of the actions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-22T12:28:46.229095Z",
          "iopub.status.busy": "2023-12-22T12:28:46.228467Z",
          "iopub.status.idle": "2023-12-22T12:28:46.233062Z",
          "shell.execute_reply": "2023-12-22T12:28:46.232447Z"
        },
        "id": "yq7JE8IwFe0E"
      },
      "outputs": [],
      "source": [
        "#main policy that is used for evaluation and deployment.\n",
        "tf_eval_policy = tf_agent.policy\n",
        "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(tf_eval_policy, use_tf_function=True)\n",
        "\n",
        "\n",
        "#policy that is used for data collection.\n",
        "tf_collect_policy = tf_agent.collect_policy\n",
        "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(tf_collect_policy, use_tf_function=True)\n",
        "\n",
        "\n",
        "#random policy\n",
        "random_policy = random_py_policy.RandomPyPolicy(collect_env.time_step_spec(), collect_env.action_spec())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-22T12:28:46.244645Z",
          "iopub.status.busy": "2023-12-22T12:28:46.244079Z",
          "iopub.status.idle": "2023-12-22T12:28:46.248220Z",
          "shell.execute_reply": "2023-12-22T12:28:46.247561Z"
        },
        "id": "BwY7StuMkuV4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1LMqw60Kuso"
      },
      "source": [
        "## Actors\n",
        "The actor manages interactions between a policy and an environment.\n",
        "  * The Actor components contain an instance of the environment (as `py_environment`) and a copy of the policy variables.\n",
        "  * Each Actor worker runs a sequence of data collection steps given the local values of the policy variables.\n",
        "  * Variable updates are done explicitly using the variable container client instance in the training script before calling `actor.run()`.\n",
        "  * The observed experience is written into the replay buffer in each data collection step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjE59ct9fU7W"
      },
      "source": [
        "As the Actors run data collection steps, they pass trajectories of (state, action, reward) to the observer, which caches and writes them to the Reverb replay system.\n",
        "\n",
        "We're storing trajectories for frames [(t0,t1) (t1,t2) (t2,t3), ...] because `stride_length=1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-22T12:28:46.251607Z",
          "iopub.status.busy": "2023-12-22T12:28:46.251120Z",
          "iopub.status.idle": "2023-12-22T12:28:46.254713Z",
          "shell.execute_reply": "2023-12-22T12:28:46.254115Z"
        },
        "id": "HbyGmdiNfNDc"
      },
      "outputs": [],
      "source": [
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  reverb_replay.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2,\n",
        "  stride_length=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-22T12:28:46.258252Z",
          "iopub.status.busy": "2023-12-22T12:28:46.257703Z",
          "iopub.status.idle": "2023-12-22T12:29:43.249174Z",
          "shell.execute_reply": "2023-12-22T12:29:43.248363Z"
        },
        "id": "ZGq3SY0kKwsa"
      },
      "outputs": [],
      "source": [
        "# We create an Actor with the random policy and collect experiences to seed the replay buffer with.\n",
        "initial_collect_actor = actor.Actor(\n",
        "  collect_env,\n",
        "  random_policy,\n",
        "  train_step,\n",
        "  steps_per_run=initial_collect_steps,\n",
        "  observers=[rb_observer])\n",
        "initial_collect_actor.run()\n",
        "\n",
        "\n",
        "\n",
        "#Instantiate an Actor with the collect policy to gather more experiences during training.\n",
        "env_step_metric = py_metrics.EnvironmentSteps()\n",
        "collect_actor = actor.Actor(\n",
        "  collect_env,\n",
        "  collect_policy,\n",
        "  train_step,\n",
        "  steps_per_run=1,\n",
        "  metrics=actor.collect_metrics(10),\n",
        "  summary_dir=os.path.join(tempdir, learner.TRAIN_DIR),\n",
        "  observers=[rb_observer, env_step_metric])\n",
        "\n",
        "\n",
        "\n",
        "#Create an Actor which will be used to evaluate the policy during training. We pass in actor.eval_metrics(num_eval_episodes) to log metrics later.\n",
        "eval_actor = actor.Actor(\n",
        "  eval_env,\n",
        "  eval_policy,\n",
        "  train_step,\n",
        "  episodes_per_run=num_eval_episodes,\n",
        "  metrics=actor.eval_metrics(num_eval_episodes),\n",
        "  summary_dir=os.path.join(tempdir, 'eval'),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6eBGSYiOf83"
      },
      "source": [
        "## Learners\n",
        "The Learner component contains the agent and performs gradient step updates to the policy variables using experience data from the replay buffer. After one or more training steps, the Learner can push a new set of variable values to the variable container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-22T12:29:43.494932Z",
          "iopub.status.busy": "2023-12-22T12:29:43.494353Z",
          "iopub.status.idle": "2023-12-22T12:29:59.762337Z",
          "shell.execute_reply": "2023-12-22T12:29:59.761348Z"
        },
        "id": "gi37YicSFTfF"
      },
      "outputs": [],
      "source": [
        "saved_model_dir = os.path.join(tempdir, learner.POLICY_SAVED_MODEL_DIR)\n",
        "\n",
        "# Triggers to save the agent's policy checkpoints.\n",
        "learning_triggers = [\n",
        "    triggers.PolicySavedModelTrigger(\n",
        "        saved_model_dir,\n",
        "        tf_agent,\n",
        "        train_step,\n",
        "        interval=policy_save_interval),\n",
        "    triggers.StepPerSecondLogTrigger(train_step, interval=1000),\n",
        "]\n",
        "\n",
        "agent_learner = learner.Learner(\n",
        "  tempdir,\n",
        "  train_step,\n",
        "  tf_agent,\n",
        "  experience_dataset_fn,\n",
        "  triggers=learning_triggers,\n",
        "  strategy=strategy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94rCXQtbUbXv"
      },
      "source": [
        "## Metrics and Evaluation\n",
        "\n",
        "We instantiated the eval Actor with `actor.eval_metrics` above, which creates most commonly used metrics during policy evaluation:\n",
        "* Average return. The return is the sum of rewards obtained while running a policy in an environment for an episode, and we usually average this over a few episodes.\n",
        "* Average episode length.\n",
        "\n",
        "We run the Actor to generate these metrics."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import datetime\n",
        "log_dir=\"logs/\"\n",
        "\n",
        "summary_writer = tf.summary.create_file_writer(\n",
        "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "\n",
        "\n",
        "def get_eval_metrics():\n",
        "  eval_actor.run()\n",
        "  results = {}\n",
        "  for metric in eval_actor.metrics:\n",
        "    results[metric.name] = metric.result()\n",
        "  return results\n",
        "\n",
        "\n",
        "def log_eval_metrics(step, metrics, is_training = False):\n",
        "  eval_results = (', ').join(\n",
        "      '{} = {:.6f}'.format(name, result) for name, result in metrics.items())\n",
        "  print('step = {0}: {1}'.format(step, eval_results))\n",
        "\n",
        "  if is_training:\n",
        "    with summary_writer.as_default():\n",
        "      for name, result in metrics.items():\n",
        "        tf.summary.scalar(name + \"_eval\", result, step=step)\n",
        "\n",
        "  return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "metrics = get_eval_metrics()\n",
        "log_eval_metrics(0, metrics)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eWNhYcLazIHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-22T12:29:59.771144Z",
          "iopub.status.busy": "2023-12-22T12:29:59.770696Z",
          "iopub.status.idle": "2023-12-22T12:30:21.866816Z",
          "shell.execute_reply": "2023-12-22T12:30:21.865972Z"
        },
        "id": "83iMSHUC71RG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBc9lj9VWWtZ"
      },
      "source": [
        "## Training the agent\n",
        "\n",
        "The training loop involves both collecting data from the environment and optimizing the agent's networks. Along the way, we will occasionally evaluate the agent's policy to see how we are doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-22T12:30:21.879022Z",
          "iopub.status.busy": "2023-12-22T12:30:21.878340Z",
          "iopub.status.idle": "2023-12-22T13:34:47.659379Z",
          "shell.execute_reply": "2023-12-22T13:34:47.658498Z"
        },
        "id": "0pTbJ3PeyF-u"
      },
      "outputs": [],
      "source": [
        "\n",
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {log_dir}\n",
        "\n",
        "# Reset the train step\n",
        "tf_agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = get_eval_metrics()[\"AverageReturn\"]\n",
        "returns = [avg_return]\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "  # Training.\n",
        "  collect_actor.run()\n",
        "  loss_info = agent_learner.run(iterations=1)\n",
        "\n",
        "  # Evaluating.\n",
        "  step = agent_learner.train_step_numpy\n",
        "\n",
        "  if eval_interval and step % eval_interval == 0:\n",
        "    metrics = get_eval_metrics()\n",
        "    log_eval_metrics(step, metrics, is_training = True)\n",
        "    returns.append(metrics[\"AverageReturn\"])\n",
        "\n",
        "  if log_interval and step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, loss_info.loss.numpy()))\n",
        "\n",
        "    with summary_writer.as_default():\n",
        "      tf.summary.scalar('batch_loss', loss_info.loss.numpy(), step=step)\n",
        "\n",
        "rb_observer.close()\n",
        "reverb_server.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68jNcA_TiJDq"
      },
      "source": [
        "## Visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-LWCdbbOIC"
      },
      "source": [
        "### Plots\n",
        "\n",
        "We can plot average return vs global steps to see the performance of our agent. In `Minitaur`, the reward function is based on how far the minitaur walks in 1000 steps and penalizes the energy expenditure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-22T13:34:47.664505Z",
          "iopub.status.busy": "2023-12-22T13:34:47.663828Z",
          "iopub.status.idle": "2023-12-22T13:34:47.882935Z",
          "shell.execute_reply": "2023-12-22T13:34:47.882323Z"
        },
        "id": "rXKzyGt72HS8"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "steps = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(steps, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Step')\n",
        "plt.ylim()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7-XpPP99Cy7"
      },
      "source": [
        "### Videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pGfGxSH32gn"
      },
      "source": [
        "It is helpful to visualize the performance of an agent by rendering the environment at each step. Before we do that, let us first create a function to embed videos in this colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-22T13:34:47.886532Z",
          "iopub.status.busy": "2023-12-22T13:34:47.886066Z",
          "iopub.status.idle": "2023-12-22T13:34:47.890568Z",
          "shell.execute_reply": "2023-12-22T13:34:47.889978Z"
        },
        "id": "ULaGr8pvOKbl"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"620\" height=\"470\" controls autoplay>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return tag\n",
        "\n",
        "\n",
        "\n",
        "def embed_multiple_mp4(filenames, titles=[]):\n",
        "  titles += [\"\"] * len(filenames)\n",
        "  titles = titles[:len(filenames)]\n",
        "\n",
        "  video_tags = \"\"\n",
        "  for filename, title in zip(filenames, titles):\n",
        "    div = f\"\"\"\n",
        "      <div style=\"margin-inline: 1%\";>\n",
        "        <p> {title} </p>\n",
        "        {embed_mp4(filename)}\n",
        "      </div>\n",
        "    \"\"\"\n",
        "    video_tags += div\n",
        "\n",
        "  html = f\"\"\"\n",
        "  <div style=\"display: flex; flex-direction: row; align-items: center; width: 100%; \">\n",
        "    {video_tags}\n",
        "  </div>\n",
        "  \"\"\"\n",
        "  return html\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def display_video(html_video):\n",
        "  return IPython.display.HTML(html_video)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "WsEMNahwHuE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DQ4HPCBkH0NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c_PH-pX4Pr5"
      },
      "source": [
        "The following code visualizes the agent's policy for a few episodes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-12-22T13:34:47.893864Z",
          "iopub.status.busy": "2023-12-22T13:34:47.893646Z",
          "iopub.status.idle": "2023-12-22T13:37:19.683131Z",
          "shell.execute_reply": "2023-12-22T13:37:19.682043Z"
        },
        "id": "PSgaQN1nXT-h"
      },
      "outputs": [],
      "source": [
        "video_eval_env = current_suite.load(env_name, gym_kwargs = ENV_CONFIG[\"env_kwarg\"], max_episode_steps=15000 )\n",
        "\n",
        "def gen_video(video_filename = env_name, num_episodes = 5, is_random = False, fps=60):\n",
        "  video_env = video_eval_env\n",
        "  if is_random: video_filename += \"_random\"\n",
        "  video_filename += '.mp4'\n",
        "\n",
        "  with imageio.get_writer(video_filename, fps=fps) as video:\n",
        "    for _ in range(num_episodes):\n",
        "      time_step = video_env.reset()\n",
        "      video.append_data(video_env.render())\n",
        "      while not time_step.is_last():\n",
        "        action_step = eval_actor.policy.action(time_step).action if (not is_random) else get_random_action().numpy()\n",
        "        time_step = video_env.step(action_step)\n",
        "        video.append_data(video_env.render())\n",
        "\n",
        "  return video_filename\n",
        "\n",
        "\n",
        "saved_video_fnames = [gen_video(is_random=False), gen_video(is_random=True)  ]\n",
        "\n",
        "display_video(  embed_multiple_mp4 (filenames=saved_video_fnames , titles=['trained policy Agent', 'uniform random policy Agent']   )  )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2b94bEnvKxhV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}